{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPCXgkVo3nkforQcWFoKNDi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haruka-inb/pytorch_practice/blob/main/deep_residual_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Residual Network"
      ],
      "metadata": {
        "id": "KueSWmS-8RyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementated section 4.2 of https://arxiv.org/pdf/1512.03385.pdf"
      ],
      "metadata": {
        "id": "8k0YRbAM0KhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: https://towardsdatascience.com/resnets-for-cifar-10-e63e900524e0"
      ],
      "metadata": {
        "id": "PVwfY3M9zuF6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR-qBveawXzS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# define hyper parameter\n",
        "num_epochs = 80\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# image preprocessing modules\n",
        "transform = transforms.Compose([transforms.Pad(4),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.RandomCrop(32),\n",
        "                                transforms.ToTensor(),\n",
        "                                ])\n",
        "\n",
        "# load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='/../../data', train=True,\n",
        "                                      download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='/../../data', train=False,\n",
        "                                     transform=transforms.ToTensor())\n",
        "\n",
        "# create data loader\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                           shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MzlrPnfgpT6",
        "outputId": "962c1720-f0ac-42e4-826a-5abdf47fea06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /../../data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 43529617.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /../../data/cifar-10-python.tar.gz to /../../data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a method to model 3x3 convolution\n",
        "def conv3x3(in_channels, out_channels, stride=1):\n",
        "  return nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                   stride=stride, padding=1, bias=False)\n",
        "\n",
        "# class for Residual Block\n",
        "class ResidualBlock(nn.Module):\n",
        "  # define the network\n",
        "  def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.conv2 = conv3x3(out_channels, out_channels)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "    self.downsample = downsample\n",
        "\n",
        "  # method to feed data into the network\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    if self.downsample:\n",
        "      residual = self.downsample(x)\n",
        "    out += residual\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "# class for ResNet\n",
        "class ResNet(nn.Module):\n",
        "  # define the network\n",
        "  # layers means how many blocks are packed in a layer\n",
        "  def __init__(self, block, layers, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_channels = 16\n",
        "    self.conv = conv3x3(3, 16)\n",
        "    self.bn = nn.BatchNorm2d(16)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.layer1 = self.make_layer(block, 16, layers[0])\n",
        "    self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
        "    self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
        "    self.avg_pool = nn.AvgPool2d(8)\n",
        "    self.fc = nn.Linear(64,num_classes)\n",
        "\n",
        "  # method to make a layer\n",
        "  def make_layer(self, block, out_channels, blocks, stride=1):\n",
        "    downsample = None\n",
        "    # Paper says downsample input through the first convolution\n",
        "    # in the layer 2 and 3\n",
        "    if (stride != 1) or (self.in_channels != out_channels):\n",
        "      downsample = nn.Sequential(\n",
        "                    conv3x3(self.in_channels, out_channels, stride=2),\n",
        "                    nn.BatchNorm2d(out_channels),\n",
        "                    nn.ReLU(inplace=True)\n",
        "                    )\n",
        "\n",
        "    layers = []\n",
        "    # add the first convolution layer, so it's 1 stride convolution in layer 1,\n",
        "    # but it's 2 stride in layer 2 and 3 because of downsampling\n",
        "    layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "\n",
        "    # add the rest of the convolution, so all are stride 1\n",
        "    self.in_channels = out_channels\n",
        "    for i in range(1, blocks):\n",
        "      layers.append(block(out_channels, out_channels))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "  # method to feed data into the network\n",
        "  def forward(self, x):\n",
        "    out = self.conv(x)\n",
        "    out = self.bn(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.avg_pool(out)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.fc(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "# move the model to cuda\n",
        "# let's say n=1. see paper\n",
        "model = ResNet(ResidualBlock, [2, 2, 2]).to(device)\n",
        "\n",
        "# model"
      ],
      "metadata": {
        "id": "YsSJZcwWzs6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define loss and optimizer\n",
        "criteria = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# For update learning rate (used fo decay learning rate)\n",
        "def update_lr(optimizer, lr):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "\n",
        "# train the model\n",
        "curr_lr = learning_rate\n",
        "\n",
        "for e in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "  # forward pass\n",
        "  outputs = model(images)\n",
        "  loss = criteria(outputs, labels)\n",
        "\n",
        "  # backward and optimize\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if (i+1) % 100 == 0:\n",
        "    print(\"Epoch {}/{}, Step {}/{}, training loss is {}\"\n",
        "    .format(e+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
        "\n",
        "  # decay learning rate\n",
        "  if (e+1) % 20 == 0:\n",
        "    curr_lr /= 3\n",
        "    update_lr(optimizer, curr_lr)\n",
        "\n",
        "# test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  correct, total = 0, 0\n",
        "  for images, labels in test_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    outputs = model(images)\n",
        "    _, pred = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += len(labels)\n",
        "    correct += (pred==labels).sum().item()\n",
        "\n",
        "  print(\"Accuracy is {} %\".format(100*correct/total))\n",
        "\n",
        "# save the model checkpoint\n",
        "torch.save(model.state_dict(), \"params.ckpt\")"
      ],
      "metadata": {
        "id": "KdItWyFzepWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47189742-f81d-4aee-8a0f-f6210ef32d5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400, Step 500/500, training loss is 2.6262741088867188\n",
            "Epoch 2/400, Step 500/500, training loss is 4.982049465179443\n",
            "Epoch 3/400, Step 500/500, training loss is 4.0545525550842285\n",
            "Epoch 4/400, Step 500/500, training loss is 3.0404510498046875\n",
            "Epoch 5/400, Step 500/500, training loss is 2.6977615356445312\n",
            "Epoch 6/400, Step 500/500, training loss is 2.545424699783325\n",
            "Epoch 7/400, Step 500/500, training loss is 2.687650680541992\n",
            "Epoch 8/400, Step 500/500, training loss is 2.3720755577087402\n",
            "Epoch 9/400, Step 500/500, training loss is 2.3953146934509277\n",
            "Epoch 10/400, Step 500/500, training loss is 2.3366892337799072\n",
            "Epoch 11/400, Step 500/500, training loss is 2.3307995796203613\n",
            "Epoch 12/400, Step 500/500, training loss is 2.315534830093384\n",
            "Epoch 13/400, Step 500/500, training loss is 2.3607523441314697\n",
            "Epoch 14/400, Step 500/500, training loss is 2.3104403018951416\n",
            "Epoch 15/400, Step 500/500, training loss is 2.339702844619751\n",
            "Epoch 16/400, Step 500/500, training loss is 2.2991890907287598\n",
            "Epoch 17/400, Step 500/500, training loss is 2.326066255569458\n",
            "Epoch 18/400, Step 500/500, training loss is 2.316779375076294\n",
            "Epoch 19/400, Step 500/500, training loss is 2.313351631164551\n",
            "Epoch 20/400, Step 500/500, training loss is 2.2942638397216797\n",
            "Epoch 21/400, Step 500/500, training loss is 2.2953388690948486\n",
            "Epoch 22/400, Step 500/500, training loss is 2.307600498199463\n",
            "Epoch 23/400, Step 500/500, training loss is 2.3066611289978027\n",
            "Epoch 24/400, Step 500/500, training loss is 2.2909929752349854\n",
            "Epoch 25/400, Step 500/500, training loss is 2.306471347808838\n",
            "Epoch 26/400, Step 500/500, training loss is 2.294070243835449\n",
            "Epoch 27/400, Step 500/500, training loss is 2.27890682220459\n",
            "Epoch 28/400, Step 500/500, training loss is 2.272914171218872\n",
            "Epoch 29/400, Step 500/500, training loss is 2.3053457736968994\n",
            "Epoch 30/400, Step 500/500, training loss is 2.279817819595337\n",
            "Epoch 31/400, Step 500/500, training loss is 2.2712790966033936\n",
            "Epoch 32/400, Step 500/500, training loss is 2.2669200897216797\n",
            "Epoch 33/400, Step 500/500, training loss is 2.293595552444458\n",
            "Epoch 34/400, Step 500/500, training loss is 2.2465317249298096\n",
            "Epoch 35/400, Step 500/500, training loss is 2.2827835083007812\n",
            "Epoch 36/400, Step 500/500, training loss is 2.2389681339263916\n",
            "Epoch 37/400, Step 500/500, training loss is 2.321758508682251\n",
            "Epoch 38/400, Step 500/500, training loss is 2.274005651473999\n",
            "Epoch 39/400, Step 500/500, training loss is 2.32458758354187\n",
            "Epoch 40/400, Step 500/500, training loss is 2.2905771732330322\n",
            "Epoch 41/400, Step 500/500, training loss is 2.257314920425415\n",
            "Epoch 42/400, Step 500/500, training loss is 2.269420862197876\n",
            "Epoch 43/400, Step 500/500, training loss is 2.2877559661865234\n",
            "Epoch 44/400, Step 500/500, training loss is 2.287417411804199\n",
            "Epoch 45/400, Step 500/500, training loss is 2.2433643341064453\n",
            "Epoch 46/400, Step 500/500, training loss is 2.28292179107666\n",
            "Epoch 47/400, Step 500/500, training loss is 2.2958357334136963\n",
            "Epoch 48/400, Step 500/500, training loss is 2.2389397621154785\n",
            "Epoch 49/400, Step 500/500, training loss is 2.2884268760681152\n",
            "Epoch 50/400, Step 500/500, training loss is 2.2206308841705322\n",
            "Epoch 51/400, Step 500/500, training loss is 2.2532799243927\n",
            "Epoch 52/400, Step 500/500, training loss is 2.2576498985290527\n",
            "Epoch 53/400, Step 500/500, training loss is 2.2734804153442383\n",
            "Epoch 54/400, Step 500/500, training loss is 2.294328212738037\n",
            "Epoch 55/400, Step 500/500, training loss is 2.2899811267852783\n",
            "Epoch 56/400, Step 500/500, training loss is 2.2336244583129883\n",
            "Epoch 57/400, Step 500/500, training loss is 2.270622968673706\n",
            "Epoch 58/400, Step 500/500, training loss is 2.2506680488586426\n",
            "Epoch 59/400, Step 500/500, training loss is 2.278106689453125\n",
            "Epoch 60/400, Step 500/500, training loss is 2.242755174636841\n",
            "Epoch 61/400, Step 500/500, training loss is 2.2352523803710938\n",
            "Epoch 62/400, Step 500/500, training loss is 2.234170436859131\n",
            "Epoch 63/400, Step 500/500, training loss is 2.289971113204956\n",
            "Epoch 64/400, Step 500/500, training loss is 2.296879291534424\n",
            "Epoch 65/400, Step 500/500, training loss is 2.2506046295166016\n",
            "Epoch 66/400, Step 500/500, training loss is 2.277721405029297\n",
            "Epoch 67/400, Step 500/500, training loss is 2.2455239295959473\n",
            "Epoch 68/400, Step 500/500, training loss is 2.281757354736328\n",
            "Epoch 69/400, Step 500/500, training loss is 2.270658493041992\n",
            "Epoch 70/400, Step 500/500, training loss is 2.226069688796997\n",
            "Epoch 71/400, Step 500/500, training loss is 2.2350962162017822\n",
            "Epoch 72/400, Step 500/500, training loss is 2.2804181575775146\n",
            "Epoch 73/400, Step 500/500, training loss is 2.2220253944396973\n",
            "Epoch 74/400, Step 500/500, training loss is 2.2552144527435303\n",
            "Epoch 75/400, Step 500/500, training loss is 2.2527477741241455\n",
            "Epoch 76/400, Step 500/500, training loss is 2.2843596935272217\n",
            "Epoch 77/400, Step 500/500, training loss is 2.269829511642456\n",
            "Epoch 78/400, Step 500/500, training loss is 2.2292304039001465\n",
            "Epoch 79/400, Step 500/500, training loss is 2.3087663650512695\n",
            "Epoch 80/400, Step 500/500, training loss is 2.271573543548584\n",
            "Epoch 81/400, Step 500/500, training loss is 2.228386640548706\n",
            "Epoch 82/400, Step 500/500, training loss is 2.269915819168091\n",
            "Epoch 83/400, Step 500/500, training loss is 2.2037041187286377\n",
            "Epoch 84/400, Step 500/500, training loss is 2.2580580711364746\n",
            "Epoch 85/400, Step 500/500, training loss is 2.216257095336914\n",
            "Epoch 86/400, Step 500/500, training loss is 2.208517551422119\n",
            "Epoch 87/400, Step 500/500, training loss is 2.253324508666992\n",
            "Epoch 88/400, Step 500/500, training loss is 2.3252370357513428\n",
            "Epoch 89/400, Step 500/500, training loss is 2.3133485317230225\n",
            "Epoch 90/400, Step 500/500, training loss is 2.2060933113098145\n",
            "Epoch 91/400, Step 500/500, training loss is 2.3167641162872314\n",
            "Epoch 92/400, Step 500/500, training loss is 2.2458765506744385\n",
            "Epoch 93/400, Step 500/500, training loss is 2.249129295349121\n",
            "Epoch 94/400, Step 500/500, training loss is 2.2702341079711914\n",
            "Epoch 95/400, Step 500/500, training loss is 2.2769720554351807\n",
            "Epoch 96/400, Step 500/500, training loss is 2.213132858276367\n",
            "Epoch 97/400, Step 500/500, training loss is 2.2112364768981934\n",
            "Epoch 98/400, Step 500/500, training loss is 2.2521448135375977\n",
            "Epoch 99/400, Step 500/500, training loss is 2.270676374435425\n",
            "Epoch 100/400, Step 500/500, training loss is 2.2720837593078613\n",
            "Epoch 101/400, Step 500/500, training loss is 2.2780351638793945\n",
            "Epoch 102/400, Step 500/500, training loss is 2.2718095779418945\n",
            "Epoch 103/400, Step 500/500, training loss is 2.26308536529541\n",
            "Epoch 104/400, Step 500/500, training loss is 2.2505292892456055\n",
            "Epoch 105/400, Step 500/500, training loss is 2.2503910064697266\n",
            "Epoch 106/400, Step 500/500, training loss is 2.310208320617676\n",
            "Epoch 107/400, Step 500/500, training loss is 2.2290735244750977\n",
            "Epoch 108/400, Step 500/500, training loss is 2.2576682567596436\n",
            "Epoch 109/400, Step 500/500, training loss is 2.222792863845825\n",
            "Epoch 110/400, Step 500/500, training loss is 2.2738494873046875\n",
            "Epoch 111/400, Step 500/500, training loss is 2.26088809967041\n",
            "Epoch 112/400, Step 500/500, training loss is 2.2734978199005127\n",
            "Epoch 113/400, Step 500/500, training loss is 2.2612578868865967\n",
            "Epoch 114/400, Step 500/500, training loss is 2.21799898147583\n",
            "Epoch 115/400, Step 500/500, training loss is 2.2831273078918457\n",
            "Epoch 116/400, Step 500/500, training loss is 2.20926833152771\n",
            "Epoch 117/400, Step 500/500, training loss is 2.251765251159668\n",
            "Epoch 118/400, Step 500/500, training loss is 2.276595115661621\n",
            "Epoch 119/400, Step 500/500, training loss is 2.232292890548706\n",
            "Epoch 120/400, Step 500/500, training loss is 2.2434487342834473\n",
            "Epoch 121/400, Step 500/500, training loss is 2.2798237800598145\n",
            "Epoch 122/400, Step 500/500, training loss is 2.2304375171661377\n",
            "Epoch 123/400, Step 500/500, training loss is 2.2950925827026367\n",
            "Epoch 124/400, Step 500/500, training loss is 2.218017101287842\n",
            "Epoch 125/400, Step 500/500, training loss is 2.301476240158081\n",
            "Epoch 126/400, Step 500/500, training loss is 2.2257022857666016\n",
            "Epoch 127/400, Step 500/500, training loss is 2.229313850402832\n",
            "Epoch 128/400, Step 500/500, training loss is 2.242931842803955\n",
            "Epoch 129/400, Step 500/500, training loss is 2.3056559562683105\n",
            "Epoch 130/400, Step 500/500, training loss is 2.2088608741760254\n",
            "Epoch 131/400, Step 500/500, training loss is 2.2409415245056152\n",
            "Epoch 132/400, Step 500/500, training loss is 2.2807319164276123\n",
            "Epoch 133/400, Step 500/500, training loss is 2.259871006011963\n",
            "Epoch 134/400, Step 500/500, training loss is 2.249851942062378\n",
            "Epoch 135/400, Step 500/500, training loss is 2.2542471885681152\n",
            "Epoch 136/400, Step 500/500, training loss is 2.2794203758239746\n",
            "Epoch 137/400, Step 500/500, training loss is 2.206942081451416\n",
            "Epoch 138/400, Step 500/500, training loss is 2.248642921447754\n",
            "Epoch 139/400, Step 500/500, training loss is 2.2642362117767334\n",
            "Epoch 140/400, Step 500/500, training loss is 2.1905322074890137\n",
            "Epoch 141/400, Step 500/500, training loss is 2.3232474327087402\n",
            "Epoch 142/400, Step 500/500, training loss is 2.20804762840271\n",
            "Epoch 143/400, Step 500/500, training loss is 2.2397866249084473\n",
            "Epoch 144/400, Step 500/500, training loss is 2.25541090965271\n",
            "Epoch 145/400, Step 500/500, training loss is 2.2762441635131836\n",
            "Epoch 146/400, Step 500/500, training loss is 2.3236348628997803\n",
            "Epoch 147/400, Step 500/500, training loss is 2.254412889480591\n",
            "Epoch 148/400, Step 500/500, training loss is 2.289151906967163\n",
            "Epoch 149/400, Step 500/500, training loss is 2.2613980770111084\n",
            "Epoch 150/400, Step 500/500, training loss is 2.2727019786834717\n",
            "Epoch 151/400, Step 500/500, training loss is 2.2777535915374756\n",
            "Epoch 152/400, Step 500/500, training loss is 2.2227158546447754\n",
            "Epoch 153/400, Step 500/500, training loss is 2.2635793685913086\n",
            "Epoch 154/400, Step 500/500, training loss is 2.270066738128662\n",
            "Epoch 155/400, Step 500/500, training loss is 2.2415056228637695\n",
            "Epoch 156/400, Step 500/500, training loss is 2.2135605812072754\n",
            "Epoch 157/400, Step 500/500, training loss is 2.258363723754883\n",
            "Epoch 158/400, Step 500/500, training loss is 2.2502126693725586\n",
            "Epoch 159/400, Step 500/500, training loss is 2.281797409057617\n",
            "Epoch 160/400, Step 500/500, training loss is 2.222181558609009\n",
            "Epoch 161/400, Step 500/500, training loss is 2.290201425552368\n",
            "Epoch 162/400, Step 500/500, training loss is 2.2785141468048096\n",
            "Epoch 163/400, Step 500/500, training loss is 2.2827963829040527\n",
            "Epoch 164/400, Step 500/500, training loss is 2.2645668983459473\n",
            "Epoch 165/400, Step 500/500, training loss is 2.2375011444091797\n",
            "Epoch 166/400, Step 500/500, training loss is 2.214235782623291\n",
            "Epoch 167/400, Step 500/500, training loss is 2.2596254348754883\n",
            "Epoch 168/400, Step 500/500, training loss is 2.2541446685791016\n",
            "Epoch 169/400, Step 500/500, training loss is 2.258030652999878\n",
            "Epoch 170/400, Step 500/500, training loss is 2.3359386920928955\n",
            "Epoch 171/400, Step 500/500, training loss is 2.192117691040039\n",
            "Epoch 172/400, Step 500/500, training loss is 2.232651710510254\n",
            "Epoch 173/400, Step 500/500, training loss is 2.2617688179016113\n",
            "Epoch 174/400, Step 500/500, training loss is 2.2349414825439453\n",
            "Epoch 175/400, Step 500/500, training loss is 2.232161521911621\n",
            "Epoch 176/400, Step 500/500, training loss is 2.2560336589813232\n",
            "Epoch 177/400, Step 500/500, training loss is 2.2562570571899414\n",
            "Epoch 178/400, Step 500/500, training loss is 2.2321763038635254\n",
            "Epoch 179/400, Step 500/500, training loss is 2.229717969894409\n",
            "Epoch 180/400, Step 500/500, training loss is 2.244924545288086\n",
            "Epoch 181/400, Step 500/500, training loss is 2.2411959171295166\n",
            "Epoch 182/400, Step 500/500, training loss is 2.2664902210235596\n",
            "Epoch 183/400, Step 500/500, training loss is 2.206122875213623\n",
            "Epoch 184/400, Step 500/500, training loss is 2.2729532718658447\n",
            "Epoch 185/400, Step 500/500, training loss is 2.2736849784851074\n",
            "Epoch 186/400, Step 500/500, training loss is 2.2485010623931885\n",
            "Epoch 187/400, Step 500/500, training loss is 2.281466007232666\n",
            "Epoch 188/400, Step 500/500, training loss is 2.273427963256836\n",
            "Epoch 189/400, Step 500/500, training loss is 2.2971391677856445\n",
            "Epoch 190/400, Step 500/500, training loss is 2.310760021209717\n",
            "Epoch 191/400, Step 500/500, training loss is 2.2540132999420166\n",
            "Epoch 192/400, Step 500/500, training loss is 2.2523703575134277\n",
            "Epoch 193/400, Step 500/500, training loss is 2.234105110168457\n",
            "Epoch 194/400, Step 500/500, training loss is 2.234142541885376\n",
            "Epoch 195/400, Step 500/500, training loss is 2.243368148803711\n",
            "Epoch 196/400, Step 500/500, training loss is 2.256152391433716\n",
            "Epoch 197/400, Step 500/500, training loss is 2.2296512126922607\n",
            "Epoch 198/400, Step 500/500, training loss is 2.3123984336853027\n",
            "Epoch 199/400, Step 500/500, training loss is 2.2711551189422607\n",
            "Epoch 200/400, Step 500/500, training loss is 2.2470545768737793\n",
            "Epoch 201/400, Step 500/500, training loss is 2.2438957691192627\n",
            "Epoch 202/400, Step 500/500, training loss is 2.2684624195098877\n",
            "Epoch 203/400, Step 500/500, training loss is 2.288539171218872\n",
            "Epoch 204/400, Step 500/500, training loss is 2.2442803382873535\n",
            "Epoch 205/400, Step 500/500, training loss is 2.256396770477295\n",
            "Epoch 206/400, Step 500/500, training loss is 2.2347357273101807\n",
            "Epoch 207/400, Step 500/500, training loss is 2.2925243377685547\n",
            "Epoch 208/400, Step 500/500, training loss is 2.201608419418335\n",
            "Epoch 209/400, Step 500/500, training loss is 2.268867015838623\n",
            "Epoch 210/400, Step 500/500, training loss is 2.275146007537842\n",
            "Epoch 211/400, Step 500/500, training loss is 2.2073841094970703\n",
            "Epoch 212/400, Step 500/500, training loss is 2.2281088829040527\n",
            "Epoch 213/400, Step 500/500, training loss is 2.2910945415496826\n",
            "Epoch 214/400, Step 500/500, training loss is 2.2951345443725586\n",
            "Epoch 215/400, Step 500/500, training loss is 2.2078707218170166\n",
            "Epoch 216/400, Step 500/500, training loss is 2.2647690773010254\n",
            "Epoch 217/400, Step 500/500, training loss is 2.229429244995117\n",
            "Epoch 218/400, Step 500/500, training loss is 2.234631061553955\n",
            "Epoch 219/400, Step 500/500, training loss is 2.215994358062744\n",
            "Epoch 220/400, Step 500/500, training loss is 2.190645933151245\n",
            "Epoch 221/400, Step 500/500, training loss is 2.2929608821868896\n",
            "Epoch 222/400, Step 500/500, training loss is 2.218550205230713\n",
            "Epoch 223/400, Step 500/500, training loss is 2.2524008750915527\n",
            "Epoch 224/400, Step 500/500, training loss is 2.2431604862213135\n",
            "Epoch 225/400, Step 500/500, training loss is 2.2519922256469727\n",
            "Epoch 226/400, Step 500/500, training loss is 2.2314095497131348\n",
            "Epoch 227/400, Step 500/500, training loss is 2.3038625717163086\n",
            "Epoch 228/400, Step 500/500, training loss is 2.2265872955322266\n",
            "Epoch 229/400, Step 500/500, training loss is 2.206875801086426\n",
            "Epoch 230/400, Step 500/500, training loss is 2.230833053588867\n",
            "Epoch 231/400, Step 500/500, training loss is 2.2679059505462646\n",
            "Epoch 232/400, Step 500/500, training loss is 2.2525603771209717\n",
            "Epoch 233/400, Step 500/500, training loss is 2.263793706893921\n",
            "Epoch 234/400, Step 500/500, training loss is 2.2628560066223145\n",
            "Epoch 235/400, Step 500/500, training loss is 2.2616217136383057\n",
            "Epoch 236/400, Step 500/500, training loss is 2.287912368774414\n",
            "Epoch 237/400, Step 500/500, training loss is 2.2478725910186768\n",
            "Epoch 238/400, Step 500/500, training loss is 2.222545862197876\n",
            "Epoch 239/400, Step 500/500, training loss is 2.2338647842407227\n",
            "Epoch 240/400, Step 500/500, training loss is 2.2532663345336914\n",
            "Epoch 241/400, Step 500/500, training loss is 2.264720916748047\n",
            "Epoch 242/400, Step 500/500, training loss is 2.265899658203125\n",
            "Epoch 243/400, Step 500/500, training loss is 2.24930477142334\n",
            "Epoch 244/400, Step 500/500, training loss is 2.2569172382354736\n",
            "Epoch 245/400, Step 500/500, training loss is 2.249006509780884\n",
            "Epoch 246/400, Step 500/500, training loss is 2.2127394676208496\n",
            "Epoch 247/400, Step 500/500, training loss is 2.280886650085449\n",
            "Epoch 248/400, Step 500/500, training loss is 2.2307639122009277\n",
            "Epoch 249/400, Step 500/500, training loss is 2.2323222160339355\n",
            "Epoch 250/400, Step 500/500, training loss is 2.273205280303955\n",
            "Epoch 251/400, Step 500/500, training loss is 2.2461681365966797\n",
            "Epoch 252/400, Step 500/500, training loss is 2.217592716217041\n",
            "Epoch 253/400, Step 500/500, training loss is 2.233220100402832\n",
            "Epoch 254/400, Step 500/500, training loss is 2.2645413875579834\n",
            "Epoch 255/400, Step 500/500, training loss is 2.2232396602630615\n",
            "Epoch 256/400, Step 500/500, training loss is 2.266777992248535\n",
            "Epoch 257/400, Step 500/500, training loss is 2.2572011947631836\n",
            "Epoch 258/400, Step 500/500, training loss is 2.255101203918457\n",
            "Epoch 259/400, Step 500/500, training loss is 2.2053632736206055\n",
            "Epoch 260/400, Step 500/500, training loss is 2.2345523834228516\n",
            "Epoch 261/400, Step 500/500, training loss is 2.2514495849609375\n",
            "Epoch 262/400, Step 500/500, training loss is 2.252587080001831\n",
            "Epoch 263/400, Step 500/500, training loss is 2.224439859390259\n",
            "Epoch 264/400, Step 500/500, training loss is 2.2633471488952637\n",
            "Epoch 265/400, Step 500/500, training loss is 2.2500851154327393\n",
            "Epoch 266/400, Step 500/500, training loss is 2.2467801570892334\n",
            "Epoch 267/400, Step 500/500, training loss is 2.2706916332244873\n",
            "Epoch 268/400, Step 500/500, training loss is 2.239254951477051\n",
            "Epoch 269/400, Step 500/500, training loss is 2.2614684104919434\n",
            "Epoch 270/400, Step 500/500, training loss is 2.2613773345947266\n",
            "Epoch 271/400, Step 500/500, training loss is 2.3024094104766846\n",
            "Epoch 272/400, Step 500/500, training loss is 2.280663013458252\n",
            "Epoch 273/400, Step 500/500, training loss is 2.2626798152923584\n",
            "Epoch 274/400, Step 500/500, training loss is 2.2396597862243652\n",
            "Epoch 275/400, Step 500/500, training loss is 2.3297805786132812\n",
            "Epoch 276/400, Step 500/500, training loss is 2.276871681213379\n",
            "Epoch 277/400, Step 500/500, training loss is 2.2564382553100586\n",
            "Epoch 278/400, Step 500/500, training loss is 2.283522844314575\n",
            "Epoch 279/400, Step 500/500, training loss is 2.2303009033203125\n",
            "Epoch 280/400, Step 500/500, training loss is 2.2758002281188965\n",
            "Epoch 281/400, Step 500/500, training loss is 2.278977394104004\n",
            "Epoch 282/400, Step 500/500, training loss is 2.2486934661865234\n",
            "Epoch 283/400, Step 500/500, training loss is 2.259976387023926\n",
            "Epoch 284/400, Step 500/500, training loss is 2.2654714584350586\n",
            "Epoch 285/400, Step 500/500, training loss is 2.268888235092163\n",
            "Epoch 286/400, Step 500/500, training loss is 2.2589738368988037\n",
            "Epoch 287/400, Step 500/500, training loss is 2.243408441543579\n",
            "Epoch 288/400, Step 500/500, training loss is 2.2390549182891846\n",
            "Epoch 289/400, Step 500/500, training loss is 2.256192684173584\n",
            "Epoch 290/400, Step 500/500, training loss is 2.2880306243896484\n",
            "Epoch 291/400, Step 500/500, training loss is 2.2515506744384766\n",
            "Epoch 292/400, Step 500/500, training loss is 2.192625045776367\n",
            "Epoch 293/400, Step 500/500, training loss is 2.2927489280700684\n",
            "Epoch 294/400, Step 500/500, training loss is 2.2400457859039307\n",
            "Epoch 295/400, Step 500/500, training loss is 2.2825868129730225\n",
            "Epoch 296/400, Step 500/500, training loss is 2.2693724632263184\n",
            "Epoch 297/400, Step 500/500, training loss is 2.277867078781128\n",
            "Epoch 298/400, Step 500/500, training loss is 2.262425661087036\n",
            "Epoch 299/400, Step 500/500, training loss is 2.246147871017456\n",
            "Epoch 300/400, Step 500/500, training loss is 2.246129035949707\n",
            "Epoch 301/400, Step 500/500, training loss is 2.2345285415649414\n",
            "Epoch 302/400, Step 500/500, training loss is 2.255706548690796\n",
            "Epoch 303/400, Step 500/500, training loss is 2.2421882152557373\n",
            "Epoch 304/400, Step 500/500, training loss is 2.24714732170105\n",
            "Epoch 305/400, Step 500/500, training loss is 2.2659358978271484\n",
            "Epoch 306/400, Step 500/500, training loss is 2.2457833290100098\n",
            "Epoch 307/400, Step 500/500, training loss is 2.209632635116577\n",
            "Epoch 308/400, Step 500/500, training loss is 2.2349326610565186\n",
            "Epoch 309/400, Step 500/500, training loss is 2.2772481441497803\n"
          ]
        }
      ]
    }
  ]
}