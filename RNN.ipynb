{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNd3NnTmN99l70Psj/y6vJE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haruka-inb/pytorch_practice/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN"
      ],
      "metadata": {
        "id": "oZqXIDG-MB72"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0orcoT-L-Ql"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# define hyoer parameters\n",
        "sequence_length = 28\n",
        "input_size = 28\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "num_epochs = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root=\"/../../data\", train=True,\n",
        "                                      download=True, transform=transform)\n",
        "testset = torchvision.datasets.MNIST(root=\"/../../data\", train=False,\n",
        "                                     download=True, transform=transform)\n",
        "\n",
        "# create Data loader\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                           shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "jWULixNTMGAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define RNN (many-to-one) class\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, num_classes=10):\n",
        "    super(RNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  # forward method\n",
        "  def forward(self, x):\n",
        "    # set initial hidden status (short-term memory) and cell status (long-term memory)\n",
        "    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "    c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "    # forward propagate LSTM\n",
        "    out, _ = self.lstm(x, (h0,c0))\n",
        "\n",
        "    # decide the hidden state of the last time step\n",
        "    out = self.fc(out[:,-1,:]) # because output from LSTM is (batch_size, seq_len, hidden_size)\n",
        "\n",
        "    return out\n",
        "\n",
        "# batch_first:\n",
        "# If your input data is of shape (batch_size, seq_len, features)\n",
        "# then you need batch_first=True and your LSTM will give output of shape (batch_size, seq_len, hidden_size).\n",
        "# Without batch_first=True it will use the first dimension as the sequence dimension.\n",
        "# With batch_first=True it will use the second dimension as the sequence dimension.\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uybyhS9X0wcu",
        "outputId": "0e20c7a9-6102-44d0-96a8-acf9dd7f5f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (lstm): LSTM(28, 128, num_layers=2, batch_first=True)\n",
              "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# train the model\n",
        "for e in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    images = images.view(-1, sequence_length, input_size).to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "  # forward pass\n",
        "  outputs = model(images)\n",
        "  loss = criterion(outputs, labels)\n",
        "\n",
        "  # backward and optimize\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if (i+1) % 100 == 0:\n",
        "    print(\"Epochs {}/{}, Steps {}/{}: Training loss is {}\"\n",
        "    .format(e+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
        "\n",
        "# test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  correct, total = 0, 0\n",
        "  for images, labels in test_loader:\n",
        "    images = images.view(-1, sequence_length, input_size).to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outputs = model(images)\n",
        "    _, pred = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (pred==labels).sum().item()\n",
        "\n",
        "  print(\"Test Accuracy of the model on the 10000 test images: {} %\"\n",
        "  .format(100*correct/total))\n",
        "\n",
        "# save the model checkpoint\n",
        "torch.save(model.state_dict(), \"model.ckpt\")"
      ],
      "metadata": {
        "id": "Joe5slHV0yvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d057282-b434-4c89-8dfd-89a9bc472ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs 1/100, Steps 600/600: Training loss is 2.3019378185272217\n",
            "Epochs 2/100, Steps 600/600: Training loss is 2.3254575729370117\n",
            "Epochs 3/100, Steps 600/600: Training loss is 2.287538528442383\n",
            "Epochs 4/100, Steps 600/600: Training loss is 2.2600953578948975\n",
            "Epochs 5/100, Steps 600/600: Training loss is 2.176805019378662\n",
            "Epochs 6/100, Steps 600/600: Training loss is 2.4118549823760986\n",
            "Epochs 7/100, Steps 600/600: Training loss is 2.107081890106201\n",
            "Epochs 8/100, Steps 600/600: Training loss is 2.0793709754943848\n",
            "Epochs 9/100, Steps 600/600: Training loss is 2.071587324142456\n",
            "Epochs 10/100, Steps 600/600: Training loss is 2.113895893096924\n",
            "Epochs 11/100, Steps 600/600: Training loss is 2.0474538803100586\n",
            "Epochs 12/100, Steps 600/600: Training loss is 1.8463839292526245\n",
            "Epochs 13/100, Steps 600/600: Training loss is 1.7465368509292603\n",
            "Epochs 14/100, Steps 600/600: Training loss is 1.7915867567062378\n",
            "Epochs 15/100, Steps 600/600: Training loss is 1.8037896156311035\n",
            "Epochs 16/100, Steps 600/600: Training loss is 1.8071790933609009\n",
            "Epochs 17/100, Steps 600/600: Training loss is 1.6946158409118652\n",
            "Epochs 18/100, Steps 600/600: Training loss is 1.6492729187011719\n",
            "Epochs 19/100, Steps 600/600: Training loss is 1.561306357383728\n",
            "Epochs 20/100, Steps 600/600: Training loss is 1.6112473011016846\n",
            "Epochs 21/100, Steps 600/600: Training loss is 1.3995082378387451\n",
            "Epochs 22/100, Steps 600/600: Training loss is 1.3922549486160278\n",
            "Epochs 23/100, Steps 600/600: Training loss is 1.4886912107467651\n",
            "Epochs 24/100, Steps 600/600: Training loss is 1.4957715272903442\n",
            "Epochs 25/100, Steps 600/600: Training loss is 1.10772705078125\n",
            "Epochs 26/100, Steps 600/600: Training loss is 1.1935745477676392\n",
            "Epochs 27/100, Steps 600/600: Training loss is 1.242830514907837\n",
            "Epochs 28/100, Steps 600/600: Training loss is 1.2768256664276123\n",
            "Epochs 29/100, Steps 600/600: Training loss is 1.0386295318603516\n",
            "Epochs 30/100, Steps 600/600: Training loss is 1.2998595237731934\n",
            "Epochs 31/100, Steps 600/600: Training loss is 1.2378538846969604\n",
            "Epochs 32/100, Steps 600/600: Training loss is 1.3139793872833252\n",
            "Epochs 33/100, Steps 600/600: Training loss is 1.2348732948303223\n",
            "Epochs 34/100, Steps 600/600: Training loss is 1.0180916786193848\n",
            "Epochs 35/100, Steps 600/600: Training loss is 1.0261387825012207\n",
            "Epochs 36/100, Steps 600/600: Training loss is 1.050281047821045\n",
            "Epochs 37/100, Steps 600/600: Training loss is 1.3147884607315063\n",
            "Epochs 38/100, Steps 600/600: Training loss is 1.2111215591430664\n",
            "Epochs 39/100, Steps 600/600: Training loss is 1.1526908874511719\n",
            "Epochs 40/100, Steps 600/600: Training loss is 1.1174688339233398\n",
            "Epochs 41/100, Steps 600/600: Training loss is 1.0652061700820923\n",
            "Epochs 42/100, Steps 600/600: Training loss is 0.915884792804718\n",
            "Epochs 43/100, Steps 600/600: Training loss is 1.0003145933151245\n",
            "Epochs 44/100, Steps 600/600: Training loss is 1.0180280208587646\n",
            "Epochs 45/100, Steps 600/600: Training loss is 1.067610740661621\n",
            "Epochs 46/100, Steps 600/600: Training loss is 1.178348183631897\n",
            "Epochs 47/100, Steps 600/600: Training loss is 0.9539121389389038\n",
            "Epochs 48/100, Steps 600/600: Training loss is 0.8703183531761169\n",
            "Epochs 49/100, Steps 600/600: Training loss is 1.0799881219863892\n",
            "Epochs 50/100, Steps 600/600: Training loss is 1.352150559425354\n",
            "Epochs 51/100, Steps 600/600: Training loss is 0.8205116391181946\n",
            "Epochs 52/100, Steps 600/600: Training loss is 1.250403881072998\n",
            "Epochs 53/100, Steps 600/600: Training loss is 0.9630678296089172\n",
            "Epochs 54/100, Steps 600/600: Training loss is 0.9767794013023376\n",
            "Epochs 55/100, Steps 600/600: Training loss is 1.020511269569397\n",
            "Epochs 56/100, Steps 600/600: Training loss is 0.9857887029647827\n",
            "Epochs 57/100, Steps 600/600: Training loss is 0.9621277451515198\n",
            "Epochs 58/100, Steps 600/600: Training loss is 0.8897663950920105\n",
            "Epochs 59/100, Steps 600/600: Training loss is 0.6965969204902649\n",
            "Epochs 60/100, Steps 600/600: Training loss is 0.801852285861969\n",
            "Epochs 61/100, Steps 600/600: Training loss is 0.984580397605896\n",
            "Epochs 62/100, Steps 600/600: Training loss is 0.67930006980896\n",
            "Epochs 63/100, Steps 600/600: Training loss is 1.0355075597763062\n",
            "Epochs 64/100, Steps 600/600: Training loss is 0.6930282115936279\n",
            "Epochs 65/100, Steps 600/600: Training loss is 0.6518299579620361\n",
            "Epochs 66/100, Steps 600/600: Training loss is 0.6220731735229492\n",
            "Epochs 67/100, Steps 600/600: Training loss is 0.7597130537033081\n",
            "Epochs 68/100, Steps 600/600: Training loss is 0.5938119292259216\n",
            "Epochs 69/100, Steps 600/600: Training loss is 0.694928765296936\n",
            "Epochs 70/100, Steps 600/600: Training loss is 0.549649715423584\n",
            "Epochs 71/100, Steps 600/600: Training loss is 0.8852255940437317\n",
            "Epochs 72/100, Steps 600/600: Training loss is 0.6226180791854858\n",
            "Epochs 73/100, Steps 600/600: Training loss is 0.7155860066413879\n",
            "Epochs 74/100, Steps 600/600: Training loss is 0.7992204427719116\n",
            "Epochs 75/100, Steps 600/600: Training loss is 0.5458258986473083\n",
            "Epochs 76/100, Steps 600/600: Training loss is 0.5901457071304321\n",
            "Epochs 77/100, Steps 600/600: Training loss is 0.7325965762138367\n",
            "Epochs 78/100, Steps 600/600: Training loss is 0.7496240139007568\n",
            "Epochs 79/100, Steps 600/600: Training loss is 0.7805261015892029\n",
            "Epochs 80/100, Steps 600/600: Training loss is 0.6765331029891968\n",
            "Epochs 81/100, Steps 600/600: Training loss is 0.8407837152481079\n",
            "Epochs 82/100, Steps 600/600: Training loss is 0.7404358386993408\n",
            "Epochs 83/100, Steps 600/600: Training loss is 0.6273557543754578\n",
            "Epochs 84/100, Steps 600/600: Training loss is 0.7045881748199463\n",
            "Epochs 85/100, Steps 600/600: Training loss is 0.6005637645721436\n",
            "Epochs 86/100, Steps 600/600: Training loss is 0.722744345664978\n",
            "Epochs 87/100, Steps 600/600: Training loss is 0.7296163439750671\n",
            "Epochs 88/100, Steps 600/600: Training loss is 0.6776261925697327\n",
            "Epochs 89/100, Steps 600/600: Training loss is 0.5016943216323853\n",
            "Epochs 90/100, Steps 600/600: Training loss is 0.5118440389633179\n",
            "Epochs 91/100, Steps 600/600: Training loss is 0.7741003632545471\n",
            "Epochs 92/100, Steps 600/600: Training loss is 0.5407521724700928\n",
            "Epochs 93/100, Steps 600/600: Training loss is 0.4877737760543823\n",
            "Epochs 94/100, Steps 600/600: Training loss is 0.5942001342773438\n",
            "Epochs 95/100, Steps 600/600: Training loss is 0.5604268312454224\n",
            "Epochs 96/100, Steps 600/600: Training loss is 0.6302284598350525\n",
            "Epochs 97/100, Steps 600/600: Training loss is 0.3405695855617523\n",
            "Epochs 98/100, Steps 600/600: Training loss is 0.5505297183990479\n",
            "Epochs 99/100, Steps 600/600: Training loss is 0.37196701765060425\n",
            "Epochs 100/100, Steps 600/600: Training loss is 0.44439074397087097\n",
            "Test Accuracy of the model on the 10000 test images: 83.79 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XayEFzfcRjj7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}