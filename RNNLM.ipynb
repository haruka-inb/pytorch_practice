{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvfrFvU5EsmoyQd8RBIsap",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haruka-inb/pytorch_practice/blob/main/RNNLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN-based Language Model"
      ],
      "metadata": {
        "id": "Qj81h6ckNN24"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7sPTFX6NKUo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch. __version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGosDffSFb7-",
        "outputId": "e2b266f3-9972-4051-81fb-ae6eca96bae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dictionary(object):\n",
        "  \"\"\"\n",
        "  This class maps words to index and te index to the words.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.idx = 0\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if not word in self.word2idx:\n",
        "      self.word2idx[word] = self.idx\n",
        "      self.idx2word[self.idx]= word\n",
        "      self.idx += 1\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.word2idx)\n",
        "\n",
        "\n",
        "class Corpus():\n",
        "  \"\"\"\n",
        "  This function reads text, tokenizes them, quantize them,\n",
        "  splits them into row by batch_size, and returns it as a tensor matrices\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.dictionary = Dictionary()\n",
        "\n",
        "  def get_data(self, path, batch_size=20):\n",
        "\n",
        "    # add words to the dictionary\n",
        "    with open(path, 'r') as f:\n",
        "      tokens = 0\n",
        "      for line in f:\n",
        "        words = line.split()  + ['<eos>'] # split passage by <eos> and word-level\n",
        "        tokens += len(words) # count number of words\n",
        "        for word in words:\n",
        "          self.dictionary.add_word(word)\n",
        "\n",
        "    # tokenize the file content\n",
        "    ids = torch.LongTensor(tokens)\n",
        "    token = 0\n",
        "    with open(path, 'r') as f:\n",
        "      for line in f:\n",
        "        words = line.split() + ['<eos>']\n",
        "        for word in words:\n",
        "          ids[token] = self.dictionary.word2idx[word] # assign the index that I mapped to the tokenized words as tensor\n",
        "          token += 1\n",
        "\n",
        "    # reshape the long sequence by batch_size e.g. tensor (929589) -> (20, 46479)\n",
        "    num_batches = ids.size(0) // batch_size\n",
        "    ids = ids[:num_batches*batch_size]\n",
        "    return ids.view(batch_size, -1)"
      ],
      "metadata": {
        "id": "Bg0bgMDwRCSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# hyper parameter\n",
        "embed_size = 128\n",
        "hidden_size = 1024\n",
        "num_layers = 1\n",
        "num_epochs = 100\n",
        "num_samples = 1000\n",
        "batch_size = 20\n",
        "seq_length = 30\n",
        "learning_rate = 0.002\n",
        "\n",
        "# load \"Penn Treebank\" dataset\n",
        "path = \"/content/train.txt\"\n",
        "corpus = Corpus()\n",
        "ids = corpus.get_data(path, batch_size)\n",
        "vocab_size = len(corpus.dictionary)\n",
        "num_batches = ids.size(1) // seq_length\n",
        "\n",
        "print(f\"unique number of vocaburaries in a text: {vocab_size}\")\n",
        "print(f\"shape of curpos: {ids.shape}\")\n",
        "print(f\"number of batches: {num_batches}\")"
      ],
      "metadata": {
        "id": "Ou2lYx7YNV6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76de0880-bcc6-4594-e171-f43342b36763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique number of vocaburaries in a text: 10000\n",
            "shape of curpos: torch.Size([20, 46479])\n",
            "number of batches: 1549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what nn.Embedding() does is one-hot encoding using a linear layer.\n",
        "# Instead giving a big one hot encoing vector, it gives the index where the 1 is mapped.\n",
        "\n",
        "# RNN based language model\n",
        "class RNNLM(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "    super(RNNLM, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  # forward method\n",
        "  def forward(self, x, h):\n",
        "    # embed word ids to vectors\n",
        "    x = self.embed(x)\n",
        "\n",
        "    # forward propagate LSTM\n",
        "    out, (h, c) = self.lstm(x, h)\n",
        "\n",
        "    # reshape output to (batch_size*sequence_length, hidden_size)\n",
        "    out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
        "\n",
        "    # decode hidden states of all time steps\n",
        "    out = self.linear(out)\n",
        "\n",
        "    return out, (h,c)"
      ],
      "metadata": {
        "id": "IWChIkhRy-nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNHTwycZnqbK",
        "outputId": "5e96212b-e582-48a8-d6ad-6c6b33e66f15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLM(\n",
              "  (embed): Embedding(10000, 128)\n",
              "  (lstm): LSTM(128, 1024, batch_first=True)\n",
              "  (linear): Linear(in_features=1024, out_features=10000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
        "\n",
        "# loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# truncated backpropagation\n",
        "# propagate loss by rolling out until T time, not entire cell\n",
        "def detach(states):\n",
        "  return [state.detach() for state in states]\n",
        "\n",
        "# train the model\n",
        "for e in range(num_epochs):\n",
        "  # set initial hidden and cell states\n",
        "  states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
        "           torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
        "\n",
        "  for i in range(0, ids.size(1) - seq_length, seq_length):\n",
        "    # get mini-batch inputs and targets\n",
        "    # model learns what the next charatcer comes, so the target is the next character of input\n",
        "    inputs = ids[:, i:i+seq_length].to(device)\n",
        "    targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
        "\n",
        "    # forward pass\n",
        "    states = detach(states)\n",
        "    outputs, states = model(inputs, states)\n",
        "    loss = criterion(outputs, targets.reshape(-1)) # RNNLM outputs 1d while targets 2d, s reshape it\n",
        "\n",
        "    # backward and optimizer\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    clip_grad_norm_(model.parameters(), 0.5) # rescale gradients if it's out of the threshold\n",
        "    optimizer.step()\n",
        "\n",
        "    step = (i+1) // seq_length\n",
        "    if step % 1500 == 0:\n",
        "      print(\"Epoch {}/{}, Step {}/{}, Loss: {:.4f}, Perplexity: {:5.2f}\"\n",
        "      .format(e+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))\n",
        "\n",
        "# test the model\n",
        "with torch.no_grad():\n",
        "  with open('sample.txt', 'w') as f:\n",
        "    # set initial hidden and cell states\n",
        "    states = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
        "              torch.zeros(num_layers, 1, hidden_size).to(device))\n",
        "\n",
        "    # select one word id randomly\n",
        "    prob = torch.ones(vocab_size)\n",
        "    input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "      # forward propagate RNN\n",
        "      output, states = model(input, states)\n",
        "\n",
        "      # sample a word id\n",
        "      prob = output.exp()\n",
        "      word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "\n",
        "      # fill input with sampled word id for the nect time step\n",
        "      input.fill_(word_id)\n",
        "\n",
        "      # file write\n",
        "      word = corpus.dictionary.idx2word[word_id]\n",
        "      word = '\\n' if word == '<eos>' else word + ''\n",
        "      f.write(word)\n",
        "\n",
        "      if (i+1) % 100 == 0:\n",
        "        print(\"Sampled {}/{} words and save to {}\"\n",
        "        .format(i+1, num_samples, 'sample.ttx'))\n",
        "\n",
        "# save the model checkpoints\n",
        "torch.save(model.state_dict(), \"model.ckpt\")"
      ],
      "metadata": {
        "id": "gn7ZNAnYzAGv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66905e90-37bb-445a-ff09-81a6fc553296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Step 0/1549, Loss: 9.2089, Perplexity: 9985.80\n",
            "Epoch 1/100, Step 1500/1549, Loss: 5.1501, Perplexity: 172.45\n",
            "Epoch 2/100, Step 0/1549, Loss: 5.4171, Perplexity: 225.24\n",
            "Epoch 2/100, Step 1500/1549, Loss: 4.3822, Perplexity: 80.01\n",
            "Epoch 3/100, Step 0/1549, Loss: 4.3853, Perplexity: 80.26\n",
            "Epoch 3/100, Step 1500/1549, Loss: 3.6564, Perplexity: 38.72\n",
            "Epoch 4/100, Step 0/1549, Loss: 3.6055, Perplexity: 36.80\n",
            "Epoch 4/100, Step 1500/1549, Loss: 3.1436, Perplexity: 23.19\n",
            "Epoch 5/100, Step 0/1549, Loss: 3.0865, Perplexity: 21.90\n",
            "Epoch 5/100, Step 1500/1549, Loss: 2.8264, Perplexity: 16.88\n",
            "Epoch 6/100, Step 0/1549, Loss: 2.7846, Perplexity: 16.19\n",
            "Epoch 6/100, Step 1500/1549, Loss: 2.6163, Perplexity: 13.69\n",
            "Epoch 7/100, Step 0/1549, Loss: 2.5256, Perplexity: 12.50\n",
            "Epoch 7/100, Step 1500/1549, Loss: 2.5134, Perplexity: 12.35\n",
            "Epoch 8/100, Step 0/1549, Loss: 2.4436, Perplexity: 11.51\n",
            "Epoch 8/100, Step 1500/1549, Loss: 2.4229, Perplexity: 11.28\n",
            "Epoch 9/100, Step 0/1549, Loss: 2.2983, Perplexity:  9.96\n",
            "Epoch 9/100, Step 1500/1549, Loss: 2.4380, Perplexity: 11.45\n",
            "Epoch 10/100, Step 0/1549, Loss: 2.2375, Perplexity:  9.37\n",
            "Epoch 10/100, Step 1500/1549, Loss: 2.5231, Perplexity: 12.47\n",
            "Epoch 11/100, Step 0/1549, Loss: 2.2023, Perplexity:  9.05\n",
            "Epoch 11/100, Step 1500/1549, Loss: 2.4188, Perplexity: 11.23\n",
            "Epoch 12/100, Step 0/1549, Loss: 2.1695, Perplexity:  8.75\n",
            "Epoch 12/100, Step 1500/1549, Loss: 2.4319, Perplexity: 11.38\n",
            "Epoch 13/100, Step 0/1549, Loss: 2.1983, Perplexity:  9.01\n",
            "Epoch 13/100, Step 1500/1549, Loss: 2.5123, Perplexity: 12.33\n",
            "Epoch 14/100, Step 0/1549, Loss: 2.2611, Perplexity:  9.59\n",
            "Epoch 14/100, Step 1500/1549, Loss: 2.5299, Perplexity: 12.55\n",
            "Epoch 15/100, Step 0/1549, Loss: 2.2875, Perplexity:  9.85\n",
            "Epoch 15/100, Step 1500/1549, Loss: 2.4937, Perplexity: 12.11\n",
            "Epoch 16/100, Step 0/1549, Loss: 2.2799, Perplexity:  9.78\n",
            "Epoch 16/100, Step 1500/1549, Loss: 2.4397, Perplexity: 11.47\n",
            "Epoch 17/100, Step 0/1549, Loss: 2.2502, Perplexity:  9.49\n",
            "Epoch 17/100, Step 1500/1549, Loss: 2.5934, Perplexity: 13.37\n",
            "Epoch 18/100, Step 0/1549, Loss: 2.2869, Perplexity:  9.84\n",
            "Epoch 18/100, Step 1500/1549, Loss: 2.5772, Perplexity: 13.16\n",
            "Epoch 19/100, Step 0/1549, Loss: 2.3500, Perplexity: 10.49\n",
            "Epoch 19/100, Step 1500/1549, Loss: 2.5648, Perplexity: 13.00\n",
            "Epoch 20/100, Step 0/1549, Loss: 2.2961, Perplexity:  9.94\n",
            "Epoch 20/100, Step 1500/1549, Loss: 2.5920, Perplexity: 13.36\n",
            "Epoch 21/100, Step 0/1549, Loss: 2.4918, Perplexity: 12.08\n",
            "Epoch 21/100, Step 1500/1549, Loss: 2.5974, Perplexity: 13.43\n",
            "Epoch 22/100, Step 0/1549, Loss: 2.4035, Perplexity: 11.06\n",
            "Epoch 22/100, Step 1500/1549, Loss: 2.5636, Perplexity: 12.98\n",
            "Epoch 23/100, Step 0/1549, Loss: 2.7144, Perplexity: 15.10\n",
            "Epoch 23/100, Step 1500/1549, Loss: 2.7608, Perplexity: 15.81\n",
            "Epoch 24/100, Step 0/1549, Loss: 2.5219, Perplexity: 12.45\n",
            "Epoch 24/100, Step 1500/1549, Loss: 2.6845, Perplexity: 14.65\n",
            "Epoch 25/100, Step 0/1549, Loss: 2.5925, Perplexity: 13.36\n",
            "Epoch 25/100, Step 1500/1549, Loss: 2.8112, Perplexity: 16.63\n",
            "Epoch 26/100, Step 0/1549, Loss: 2.8472, Perplexity: 17.24\n",
            "Epoch 26/100, Step 1500/1549, Loss: 2.9045, Perplexity: 18.26\n",
            "Epoch 27/100, Step 0/1549, Loss: 2.6990, Perplexity: 14.86\n",
            "Epoch 27/100, Step 1500/1549, Loss: 2.8570, Perplexity: 17.41\n",
            "Epoch 28/100, Step 0/1549, Loss: 2.6984, Perplexity: 14.86\n",
            "Epoch 28/100, Step 1500/1549, Loss: 2.8687, Perplexity: 17.61\n",
            "Epoch 29/100, Step 0/1549, Loss: 2.8836, Perplexity: 17.88\n",
            "Epoch 29/100, Step 1500/1549, Loss: 2.9707, Perplexity: 19.51\n",
            "Epoch 30/100, Step 0/1549, Loss: 2.9902, Perplexity: 19.89\n",
            "Epoch 30/100, Step 1500/1549, Loss: 2.9039, Perplexity: 18.24\n",
            "Epoch 31/100, Step 0/1549, Loss: 3.0054, Perplexity: 20.19\n",
            "Epoch 31/100, Step 1500/1549, Loss: 3.0496, Perplexity: 21.11\n",
            "Epoch 32/100, Step 0/1549, Loss: 3.0412, Perplexity: 20.93\n",
            "Epoch 32/100, Step 1500/1549, Loss: 3.0846, Perplexity: 21.86\n",
            "Epoch 33/100, Step 0/1549, Loss: 3.0052, Perplexity: 20.19\n",
            "Epoch 33/100, Step 1500/1549, Loss: 2.9986, Perplexity: 20.06\n",
            "Epoch 34/100, Step 0/1549, Loss: 3.2511, Perplexity: 25.82\n",
            "Epoch 34/100, Step 1500/1549, Loss: 2.9978, Perplexity: 20.04\n",
            "Epoch 35/100, Step 0/1549, Loss: 3.1162, Perplexity: 22.56\n",
            "Epoch 35/100, Step 1500/1549, Loss: 3.1496, Perplexity: 23.33\n",
            "Epoch 36/100, Step 0/1549, Loss: 3.3265, Perplexity: 27.84\n",
            "Epoch 36/100, Step 1500/1549, Loss: 3.1612, Perplexity: 23.60\n",
            "Epoch 37/100, Step 0/1549, Loss: 3.2319, Perplexity: 25.33\n",
            "Epoch 37/100, Step 1500/1549, Loss: 3.1818, Perplexity: 24.09\n",
            "Epoch 38/100, Step 0/1549, Loss: 3.1918, Perplexity: 24.33\n",
            "Epoch 38/100, Step 1500/1549, Loss: 3.1419, Perplexity: 23.15\n",
            "Epoch 39/100, Step 0/1549, Loss: 3.3895, Perplexity: 29.65\n",
            "Epoch 39/100, Step 1500/1549, Loss: 3.1099, Perplexity: 22.42\n",
            "Epoch 40/100, Step 0/1549, Loss: 3.4640, Perplexity: 31.94\n",
            "Epoch 40/100, Step 1500/1549, Loss: 3.1375, Perplexity: 23.05\n",
            "Epoch 41/100, Step 0/1549, Loss: 3.5817, Perplexity: 35.93\n",
            "Epoch 41/100, Step 1500/1549, Loss: 3.0717, Perplexity: 21.58\n",
            "Epoch 42/100, Step 0/1549, Loss: 3.6374, Perplexity: 37.99\n",
            "Epoch 42/100, Step 1500/1549, Loss: 2.9784, Perplexity: 19.66\n",
            "Epoch 43/100, Step 0/1549, Loss: 3.7169, Perplexity: 41.14\n",
            "Epoch 43/100, Step 1500/1549, Loss: 3.1262, Perplexity: 22.79\n",
            "Epoch 44/100, Step 0/1549, Loss: 3.7212, Perplexity: 41.32\n",
            "Epoch 44/100, Step 1500/1549, Loss: 3.2604, Perplexity: 26.06\n",
            "Epoch 45/100, Step 0/1549, Loss: 3.6815, Perplexity: 39.71\n",
            "Epoch 45/100, Step 1500/1549, Loss: 3.1559, Perplexity: 23.47\n",
            "Epoch 46/100, Step 0/1549, Loss: 3.6556, Perplexity: 38.69\n",
            "Epoch 46/100, Step 1500/1549, Loss: 3.1591, Perplexity: 23.55\n",
            "Epoch 47/100, Step 0/1549, Loss: 3.7573, Perplexity: 42.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "\n",
        "with open(\"/content/sample.txt\", 'r') as f:\n",
        "  for line in f:\n",
        "    result.append(line.split('<eos>'))\n",
        "\n",
        "result"
      ],
      "metadata": {
        "id": "5_O4QwRcRm2S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}